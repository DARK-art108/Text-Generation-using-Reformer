{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psnUF-8c02o_"
   },
   "source": [
    "# Reformer: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lnRd_IoERdk"
   },
   "source": [
    "This notebook was designed to run on TPU.\n",
    "\n",
    "To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PluCmWbZIpJ",
    "outputId": "7dbac91d-9eae-4eef-f5a6-24255796fc74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n",
      "Requirement already up-to-date: jaxlib in /usr/local/lib/python3.6/dist-packages (0.1.57+cuda101)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib) (1.15.0)\n",
      "Requirement already up-to-date: trax in /usr/local/lib/python3.6/dist-packages (1.3.6)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-text in /usr/local/lib/python3.6/dist-packages (from trax) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.57+cuda101)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: t5 in /usr/local/lib/python3.6/dist-packages (from trax) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: funcsigs in /usr/local/lib/python3.6/dist-packages (from trax) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->trax) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: rouge-score in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.0.4)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: sacrebleu in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.4.14)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5->trax) (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.17)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.94)\n",
      "Requirement already satisfied, skipping upgrade: tfds-nightly in /usr/local/lib/python3.6/dist-packages (from t5->trax) (4.1.0.dev202012120107)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.25.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.34.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.36.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->t5->trax) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->t5->trax) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.7)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5->trax) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (50.3.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n",
      "grpc://10.86.215.66:8470\n"
     ]
    }
   ],
   "source": [
    "# Install JAX.\n",
    "!pip install --upgrade jax\n",
    "!pip install --upgrade jaxlib\n",
    "!pip install --upgrade trax\n",
    "\n",
    "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
    "import requests\n",
    "import os\n",
    "if 'TPU_DRIVER_MODE' not in globals():\n",
    "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
    "  resp = requests.post(url)\n",
    "  TPU_DRIVER_MODE = 1\n",
    "\n",
    "# The following is required to use TPU Driver as JAX's backend.\n",
    "from jax.config import config\n",
    "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
    "print(config.FLAGS.jax_backend_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yiPdBenoZwH6"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q sentencepiece\n",
    "!pip install --upgrade -q gin \n",
    "\n",
    "from tensorflow.compat.v1.io.gfile import GFile\n",
    "import gin\n",
    "import os\n",
    "import jax\n",
    "import trax\n",
    "from trax.data import inputs\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ89jHCYfhpg"
   },
   "source": [
    "## Setting up data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_OCIqghSyfs"
   },
   "source": [
    "In this notebook, we'll be pushing the limits of just how many tokens we can fit on a single TPU device. The TPUs available in Colab have 8GB of memory per core, and 8 cores. We will set up a Reformer model that can fit a copy of \"Crime and Punishment\" on *each* of the 8 TPU cores (over 500,000 tokens per 8GB of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tYSOVGR47LVL"
   },
   "outputs": [],
   "source": [
    "# Import a copy of \"Crime and Punishment\", by Fyodor Dostoevsky\n",
    "with GFile('gs://trax-ml/reformer/crime-and-punishment-2554.txt') as f:\n",
    "  text = f.read()\n",
    "\n",
    "# The file read above includes metadata and licensing information.\n",
    "# For training our language model, we will only use the actual novel text.\n",
    "start = text.find('CRIME AND PUNISHMENT')  # skip header\n",
    "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip header\n",
    "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip translator preface\n",
    "end = text.rfind('End of Project')  # skip extra text at the end\n",
    "text = text[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMntV3H-6OR0",
    "outputId": "3423ccdf-68f9-4638-a6df-5ceb9f5216dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://trax-ml/reformer/cp.320.model...\n",
      "Copying gs://trax-ml/reformer/cp.320.vocab...\n",
      "/ [2 files][239.0 KiB/239.0 KiB]                                                \n",
      "Operation completed over 2 objects/239.0 KiB.                                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a BPE vocabulaary with 320 types. This mostly consists of single letters\n",
    "# and pairs of letters, but it has some common words and word pieces, too.\n",
    "!gsutil cp gs://trax-ml/reformer/cp.320.* .\n",
    "\n",
    "TOKENIZER = SentencePieceProcessor()\n",
    "TOKENIZER.load('cp.320.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnJzxSi_77zP",
    "outputId": "3588ae20-6a34-49e1-945e-7cef3e75883d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 513812\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "IDS = TOKENIZER.EncodeAsIds(text)\n",
    "IDS = np.asarray(IDS, dtype=np.int32)\n",
    "PAD_AMOUNT = 512 * 1024 - len(IDS)\n",
    "print(\"Number of tokens:\", IDS.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzQ7G9uGSga5"
   },
   "source": [
    "As we see above, \"Crime and Punishment\" has just over half a million tokens with the BPE vocabulary we have selected.\n",
    "\n",
    "Normally we would have a dataset with many examples, but for this demonstration we fit a language model on the single novel only. We don't want the model to just memorize the dataset by encoding the words in its position embeddings, so at each training iteration we will randomly select how much padding to put before the text vs. after it.\n",
    "\n",
    "We have 8 TPU cores, so we will separately randomize the amount of padding for each core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdAwmpS220ub",
    "outputId": "de2c5c5a-ad15-40b2-9764-ab03cffcf93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(device count, tokens per device) =  (8, 524288)\n"
     ]
    }
   ],
   "source": [
    "# Set up the data pipeline.\n",
    "def my_inputs(n_devices):\n",
    "  while True:\n",
    "    inputs = []\n",
    "    mask = []\n",
    "    pad_amounts = np.random.choice(PAD_AMOUNT, n_devices)\n",
    "    for i in range(n_devices):\n",
    "      inputs.append(np.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
    "                            mode='constant'))\n",
    "      mask.append(np.pad(np.ones_like(IDS, dtype=np.float32),\n",
    "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
    "                          mode='constant'))\n",
    "    inputs = np.stack(inputs)\n",
    "    mask = np.stack(mask)\n",
    "    yield (inputs, inputs, mask)\n",
    "\n",
    "print(\"(device count, tokens per device) = \",\n",
    "      next(my_inputs(trax.fastmath.device_count()))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ei90LdK024r_",
    "outputId": "31f6c19d-c357-4c23-e8dc-5f87985655eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['trax.layers',\n",
       "  'trax.models',\n",
       "  'trax.optimizers',\n",
       "  'trax.data.inputs',\n",
       "  'trax.supervised.trainer_lib'])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure hyperparameters.\n",
    "gin.parse_config(\"\"\"\n",
    "import trax.layers\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.data.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "\n",
    "# Parameters that will vary between experiments:\n",
    "# ==============================================================================\n",
    "train.model = @trax.models.ReformerLM\n",
    "# Our model will have 6 layers, alternating between the LSH attention proposed\n",
    "# in the Reformer paper and local attention within a certain context window.\n",
    "n_layers = 6\n",
    "attn_type = [\n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,  \n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,\n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,\n",
    "  ]\n",
    "share_qk = False  # LSH attention ignores this flag and always shares q & k\n",
    "n_heads = 2\n",
    "attn_kv = 64\n",
    "dropout = 0.05\n",
    "n_tokens = 524288\n",
    "\n",
    "# Parameters for multifactor:\n",
    "# ==============================================================================\n",
    "multifactor.constant = 0.01\n",
    "multifactor.factors = 'constant * linear_warmup * cosine_decay'\n",
    "multifactor.warmup_steps = 100\n",
    "multifactor.steps_per_cycle = 900\n",
    "\n",
    "# Parameters for Adam:\n",
    "# ==============================================================================\n",
    "Adam.weight_decay_rate=0.0\n",
    "Adam.b1 = 0.86\n",
    "Adam.b2 = 0.92\n",
    "Adam.eps = 1e-9\n",
    "\n",
    "# Parameters for SelfAttention:\n",
    "# ==============================================================================\n",
    "trax.layers.SelfAttention.attention_dropout = 0.05\n",
    "trax.layers.SelfAttention.chunk_len = 64\n",
    "trax.layers.SelfAttention.n_chunks_before = 1\n",
    "trax.layers.SelfAttention.n_parallel_heads = 1\n",
    "\n",
    "# Parameters for LSHSelfAttention:\n",
    "# ==============================================================================\n",
    "LSHSelfAttention.attention_dropout = 0.0\n",
    "LSHSelfAttention.chunk_len = 64\n",
    "LSHSelfAttention.n_buckets = [64, 128]\n",
    "LSHSelfAttention.n_chunks_after = 0\n",
    "LSHSelfAttention.n_chunks_before = 1\n",
    "LSHSelfAttention.n_hashes = 1\n",
    "LSHSelfAttention.n_parallel_heads = 1\n",
    "LSHSelfAttention.predict_drop_len = 128\n",
    "LSHSelfAttention.predict_mem_len = 1024\n",
    "\n",
    "# Parameters for ReformerLM:\n",
    "# ==============================================================================\n",
    "ReformerLM.attention_type = %attn_type\n",
    "ReformerLM.d_attention_key = %attn_kv\n",
    "ReformerLM.d_attention_value = %attn_kv\n",
    "ReformerLM.d_model = 256\n",
    "ReformerLM.d_ff = 512\n",
    "ReformerLM.dropout = %dropout\n",
    "ReformerLM.ff_activation = @trax.layers.Relu\n",
    "ReformerLM.max_len = %n_tokens\n",
    "ReformerLM.mode = 'train'\n",
    "ReformerLM.n_heads = %n_heads\n",
    "ReformerLM.n_layers = %n_layers\n",
    "ReformerLM.vocab_size = 320\n",
    "ReformerLM.axial_pos_shape = (512, 1024)\n",
    "ReformerLM.d_axial_pos_embs= (64, 192)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RGGt0WaT3a-h"
   },
   "outputs": [],
   "source": [
    "# Set up a Trainer.\n",
    "output_dir = os.path.expanduser('~/train_dir/')\n",
    "!rm -f ~/train_dir/model.pkl.gz  # Remove old model\n",
    "\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.ReformerLM,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam,\n",
    "    lr_schedule=trax.lr.multifactor(),\n",
    "    inputs=trax.data.inputs.Inputs(my_inputs),\n",
    "    output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6VQkmKO3a1L",
    "outputId": "e86cde0c-337d-4032-8601-9353a6832996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Ran 1 train steps in 158.46 secs\n",
      "Step      1: Evaluation\n",
      "Step      1: train                   accuracy |  0.00292933\n",
      "Step      1: train                       loss |  6.40892601\n",
      "Step      1: train         neg_log_perplexity | -6.40892601\n",
      "Step      1: train          sequence_accuracy |  0.00000000\n",
      "Step      1: train weights_per_batch_per_core |  513812.00000000\n",
      "Step      1: eval                    accuracy |  0.00286681\n",
      "Step      1: eval                        loss |  6.40883303\n",
      "Step      1: eval          neg_log_perplexity | -6.40883303\n",
      "Step      1: eval           sequence_accuracy |  0.00000000\n",
      "Step      1: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step      1: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "# Run one training step, to make sure the model fits in memory.\n",
    "# The first time trainer.train_epoch is called, it will JIT the entire network\n",
    "# architecture, which takes around 2 minutes. The JIT-compiled model is saved\n",
    "# so subsequent runs will be much faster than the first.\n",
    "trainer.train_epoch(n_steps=1, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFnX4G6z3asD",
    "outputId": "74c2832b-c898-43af-af89-3cb875d396c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step     10: Ran 9 train steps in 187.68 secs\n",
      "Step     10: Evaluation\n",
      "Step     10: train                   accuracy |  0.02779835\n",
      "Step     10: train                       loss |  5.37467575\n",
      "Step     10: train         neg_log_perplexity | -5.37467575\n",
      "Step     10: train          sequence_accuracy |  0.00000000\n",
      "Step     10: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     10: eval                    accuracy |  0.02782243\n",
      "Step     10: eval                        loss |  5.37470198\n",
      "Step     10: eval          neg_log_perplexity | -5.37470198\n",
      "Step     10: eval           sequence_accuracy |  0.00000000\n",
      "Step     10: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     10: Finished evaluation\n",
      "\n",
      "Step     20: Ran 10 train steps in 35.91 secs\n",
      "Step     20: Evaluation\n",
      "Step     20: train                   accuracy |  0.02764411\n",
      "Step     20: train                       loss |  5.30323601\n",
      "Step     20: train         neg_log_perplexity | -5.30323601\n",
      "Step     20: train          sequence_accuracy |  0.00000000\n",
      "Step     20: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     20: eval                    accuracy |  0.02770688\n",
      "Step     20: eval                        loss |  5.30330515\n",
      "Step     20: eval          neg_log_perplexity | -5.30330515\n",
      "Step     20: eval           sequence_accuracy |  0.00000000\n",
      "Step     20: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     20: Finished evaluation\n",
      "\n",
      "Step     30: Ran 10 train steps in 35.79 secs\n",
      "Step     30: Evaluation\n",
      "Step     30: train                   accuracy |  0.02913322\n",
      "Step     30: train                       loss |  5.28357601\n",
      "Step     30: train         neg_log_perplexity | -5.28357601\n",
      "Step     30: train          sequence_accuracy |  0.00000000\n",
      "Step     30: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     30: eval                    accuracy |  0.02913322\n",
      "Step     30: eval                        loss |  5.28363514\n",
      "Step     30: eval          neg_log_perplexity | -5.28363514\n",
      "Step     30: eval           sequence_accuracy |  0.00000000\n",
      "Step     30: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     30: Finished evaluation\n",
      "\n",
      "Step     40: Ran 10 train steps in 35.73 secs\n",
      "Step     40: Evaluation\n",
      "Step     40: train                   accuracy |  0.02914028\n",
      "Step     40: train                       loss |  5.28379345\n",
      "Step     40: train         neg_log_perplexity | -5.28379345\n",
      "Step     40: train          sequence_accuracy |  0.00000000\n",
      "Step     40: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     40: eval                    accuracy |  0.02914077\n",
      "Step     40: eval                        loss |  5.28388834\n",
      "Step     40: eval          neg_log_perplexity | -5.28388834\n",
      "Step     40: eval           sequence_accuracy |  0.00000000\n",
      "Step     40: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     40: Finished evaluation\n",
      "\n",
      "Step     50: Ran 10 train steps in 35.66 secs\n",
      "Step     50: Evaluation\n",
      "Step     50: train                   accuracy |  0.02913420\n",
      "Step     50: train                       loss |  5.27518940\n",
      "Step     50: train         neg_log_perplexity | -5.27518940\n",
      "Step     50: train          sequence_accuracy |  0.00000000\n",
      "Step     50: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     50: eval                    accuracy |  0.02913371\n",
      "Step     50: eval                        loss |  5.27522945\n",
      "Step     50: eval          neg_log_perplexity | -5.27522945\n",
      "Step     50: eval           sequence_accuracy |  0.00000000\n",
      "Step     50: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     50: Finished evaluation\n",
      "\n",
      "Step     60: Ran 10 train steps in 36.38 secs\n",
      "Step     60: Evaluation\n",
      "Step     60: train                   accuracy |  0.03395235\n",
      "Step     60: train                       loss |  4.97377777\n",
      "Step     60: train         neg_log_perplexity | -4.97377777\n",
      "Step     60: train          sequence_accuracy |  0.00000000\n",
      "Step     60: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     60: eval                    accuracy |  0.03400198\n",
      "Step     60: eval                        loss |  4.97406864\n",
      "Step     60: eval          neg_log_perplexity | -4.97406864\n",
      "Step     60: eval           sequence_accuracy |  0.00000000\n",
      "Step     60: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     60: Finished evaluation\n",
      "\n",
      "Step     70: Ran 10 train steps in 35.74 secs\n",
      "Step     70: Evaluation\n",
      "Step     70: train                   accuracy |  0.05577819\n",
      "Step     70: train                       loss |  4.76839352\n",
      "Step     70: train         neg_log_perplexity | -4.76839352\n",
      "Step     70: train          sequence_accuracy |  0.00000000\n",
      "Step     70: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     70: eval                    accuracy |  0.05586455\n",
      "Step     70: eval                        loss |  4.76800251\n",
      "Step     70: eval          neg_log_perplexity | -4.76800251\n",
      "Step     70: eval           sequence_accuracy |  0.00000000\n",
      "Step     70: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     70: Finished evaluation\n",
      "\n",
      "Step     80: Ran 10 train steps in 35.90 secs\n",
      "Step     80: Evaluation\n",
      "Step     80: train                   accuracy |  0.06424408\n",
      "Step     80: train                       loss |  4.60692835\n",
      "Step     80: train         neg_log_perplexity | -4.60692835\n",
      "Step     80: train          sequence_accuracy |  0.00000000\n",
      "Step     80: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     80: eval                    accuracy |  0.06410930\n",
      "Step     80: eval                        loss |  4.60708141\n",
      "Step     80: eval          neg_log_perplexity | -4.60708141\n",
      "Step     80: eval           sequence_accuracy |  0.00000000\n",
      "Step     80: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     80: Finished evaluation\n",
      "\n",
      "Step     90: Ran 10 train steps in 35.79 secs\n",
      "Step     90: Evaluation\n",
      "Step     90: train                   accuracy |  0.06904982\n",
      "Step     90: train                       loss |  4.49170780\n",
      "Step     90: train         neg_log_perplexity | -4.49170780\n",
      "Step     90: train          sequence_accuracy |  0.00000000\n",
      "Step     90: train weights_per_batch_per_core |  513812.00000000\n",
      "Step     90: eval                    accuracy |  0.06915102\n",
      "Step     90: eval                        loss |  4.49128294\n",
      "Step     90: eval          neg_log_perplexity | -4.49128294\n",
      "Step     90: eval           sequence_accuracy |  0.00000000\n",
      "Step     90: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step     90: Finished evaluation\n",
      "\n",
      "Step    100: Ran 10 train steps in 35.79 secs\n",
      "Step    100: Evaluation\n",
      "Step    100: train                   accuracy |  0.08999280\n",
      "Step    100: train                       loss |  4.29469872\n",
      "Step    100: train         neg_log_perplexity | -4.29469872\n",
      "Step    100: train          sequence_accuracy |  0.00000000\n",
      "Step    100: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    100: eval                    accuracy |  0.08989450\n",
      "Step    100: eval                        loss |  4.29515553\n",
      "Step    100: eval          neg_log_perplexity | -4.29515553\n",
      "Step    100: eval           sequence_accuracy |  0.00000000\n",
      "Step    100: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    100: Finished evaluation\n",
      "\n",
      "Step    110: Ran 10 train steps in 35.87 secs\n",
      "Step    110: Evaluation\n",
      "Step    110: train                   accuracy |  0.09996507\n",
      "Step    110: train                       loss |  4.17378139\n",
      "Step    110: train         neg_log_perplexity | -4.17378139\n",
      "Step    110: train          sequence_accuracy |  0.00000000\n",
      "Step    110: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    110: eval                    accuracy |  0.09995534\n",
      "Step    110: eval                        loss |  4.17344999\n",
      "Step    110: eval          neg_log_perplexity | -4.17344999\n",
      "Step    110: eval           sequence_accuracy |  0.00000000\n",
      "Step    110: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    110: Finished evaluation\n",
      "\n",
      "Step    120: Ran 10 train steps in 35.78 secs\n",
      "Step    120: Evaluation\n",
      "Step    120: train                   accuracy |  0.11471025\n",
      "Step    120: train                       loss |  4.05091381\n",
      "Step    120: train         neg_log_perplexity | -4.05091381\n",
      "Step    120: train          sequence_accuracy |  0.00000000\n",
      "Step    120: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    120: eval                    accuracy |  0.11459202\n",
      "Step    120: eval                        loss |  4.05069256\n",
      "Step    120: eval          neg_log_perplexity | -4.05069256\n",
      "Step    120: eval           sequence_accuracy |  0.00000000\n",
      "Step    120: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    120: Finished evaluation\n",
      "\n",
      "Step    130: Ran 10 train steps in 35.82 secs\n",
      "Step    130: Evaluation\n",
      "Step    130: train                   accuracy |  0.12506959\n",
      "Step    130: train                       loss |  3.95006108\n",
      "Step    130: train         neg_log_perplexity | -3.95006108\n",
      "Step    130: train          sequence_accuracy |  0.00000000\n",
      "Step    130: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    130: eval                    accuracy |  0.12497860\n",
      "Step    130: eval                        loss |  3.95046043\n",
      "Step    130: eval          neg_log_perplexity | -3.95046043\n",
      "Step    130: eval           sequence_accuracy |  0.00000000\n",
      "Step    130: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    130: Finished evaluation\n",
      "\n",
      "Step    140: Ran 10 train steps in 35.88 secs\n",
      "Step    140: Evaluation\n",
      "Step    140: train                   accuracy |  0.13284723\n",
      "Step    140: train                       loss |  3.86991978\n",
      "Step    140: train         neg_log_perplexity | -3.86991978\n",
      "Step    140: train          sequence_accuracy |  0.00000000\n",
      "Step    140: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    140: eval                    accuracy |  0.13283117\n",
      "Step    140: eval                        loss |  3.87018156\n",
      "Step    140: eval          neg_log_perplexity | -3.87018156\n",
      "Step    140: eval           sequence_accuracy |  0.00000000\n",
      "Step    140: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    140: Finished evaluation\n",
      "\n",
      "Step    150: Ran 10 train steps in 35.81 secs\n",
      "Step    150: Evaluation\n",
      "Step    150: train                   accuracy |  0.13846675\n",
      "Step    150: train                       loss |  3.79115963\n",
      "Step    150: train         neg_log_perplexity | -3.79115963\n",
      "Step    150: train          sequence_accuracy |  0.00000000\n",
      "Step    150: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    150: eval                    accuracy |  0.13842101\n",
      "Step    150: eval                        loss |  3.79146910\n",
      "Step    150: eval          neg_log_perplexity | -3.79146910\n",
      "Step    150: eval           sequence_accuracy |  0.00000000\n",
      "Step    150: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    150: Finished evaluation\n",
      "\n",
      "Step    160: Ran 10 train steps in 35.81 secs\n",
      "Step    160: Evaluation\n",
      "Step    160: train                   accuracy |  0.13945836\n",
      "Step    160: train                       loss |  3.73456788\n",
      "Step    160: train         neg_log_perplexity | -3.73456788\n",
      "Step    160: train          sequence_accuracy |  0.00000000\n",
      "Step    160: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    160: eval                    accuracy |  0.13963401\n",
      "Step    160: eval                        loss |  3.73401046\n",
      "Step    160: eval          neg_log_perplexity | -3.73401046\n",
      "Step    160: eval           sequence_accuracy |  0.00000000\n",
      "Step    160: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    160: Finished evaluation\n",
      "\n",
      "Step    170: Ran 10 train steps in 35.84 secs\n",
      "Step    170: Evaluation\n",
      "Step    170: train                   accuracy |  0.14727104\n",
      "Step    170: train                       loss |  3.67925382\n",
      "Step    170: train         neg_log_perplexity | -3.67925382\n",
      "Step    170: train          sequence_accuracy |  0.00000000\n",
      "Step    170: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    170: eval                    accuracy |  0.14755738\n",
      "Step    170: eval                        loss |  3.67735410\n",
      "Step    170: eval          neg_log_perplexity | -3.67735410\n",
      "Step    170: eval           sequence_accuracy |  0.00000000\n",
      "Step    170: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    170: Finished evaluation\n",
      "\n",
      "Step    180: Ran 10 train steps in 35.79 secs\n",
      "Step    180: Evaluation\n",
      "Step    180: train                   accuracy |  0.14625560\n",
      "Step    180: train                       loss |  3.64688563\n",
      "Step    180: train         neg_log_perplexity | -3.64688563\n",
      "Step    180: train          sequence_accuracy |  0.00000000\n",
      "Step    180: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    180: eval                    accuracy |  0.14612299\n",
      "Step    180: eval                        loss |  3.64784575\n",
      "Step    180: eval          neg_log_perplexity | -3.64784575\n",
      "Step    180: eval           sequence_accuracy |  0.00000000\n",
      "Step    180: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    180: Finished evaluation\n",
      "\n",
      "Step    190: Ran 10 train steps in 35.82 secs\n",
      "Step    190: Evaluation\n",
      "Step    190: train                   accuracy |  0.15263537\n",
      "Step    190: train                       loss |  3.61048508\n",
      "Step    190: train         neg_log_perplexity | -3.61048508\n",
      "Step    190: train          sequence_accuracy |  0.00000000\n",
      "Step    190: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    190: eval                    accuracy |  0.15307374\n",
      "Step    190: eval                        loss |  3.60818577\n",
      "Step    190: eval          neg_log_perplexity | -3.60818577\n",
      "Step    190: eval           sequence_accuracy |  0.00000000\n",
      "Step    190: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    190: Finished evaluation\n",
      "\n",
      "Step    200: Ran 10 train steps in 36.16 secs\n",
      "Step    200: Evaluation\n",
      "Step    200: train                   accuracy |  0.15883607\n",
      "Step    200: train                       loss |  3.55759525\n",
      "Step    200: train         neg_log_perplexity | -3.55759525\n",
      "Step    200: train          sequence_accuracy |  0.00000000\n",
      "Step    200: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    200: eval                    accuracy |  0.15819138\n",
      "Step    200: eval                        loss |  3.55988812\n",
      "Step    200: eval          neg_log_perplexity | -3.55988812\n",
      "Step    200: eval           sequence_accuracy |  0.00000000\n",
      "Step    200: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    200: Finished evaluation\n",
      "\n",
      "Step    210: Ran 10 train steps in 35.83 secs\n",
      "Step    210: Evaluation\n",
      "Step    210: train                   accuracy |  0.16155303\n",
      "Step    210: train                       loss |  3.53978491\n",
      "Step    210: train         neg_log_perplexity | -3.53978491\n",
      "Step    210: train          sequence_accuracy |  0.00000000\n",
      "Step    210: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    210: eval                    accuracy |  0.16103216\n",
      "Step    210: eval                        loss |  3.54186273\n",
      "Step    210: eval          neg_log_perplexity | -3.54186273\n",
      "Step    210: eval           sequence_accuracy |  0.00000000\n",
      "Step    210: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    210: Finished evaluation\n",
      "\n",
      "Step    220: Ran 10 train steps in 35.83 secs\n",
      "Step    220: Evaluation\n",
      "Step    220: train                   accuracy |  0.16712412\n",
      "Step    220: train                       loss |  3.50410652\n",
      "Step    220: train         neg_log_perplexity | -3.50410652\n",
      "Step    220: train          sequence_accuracy |  0.00000000\n",
      "Step    220: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    220: eval                    accuracy |  0.16697305\n",
      "Step    220: eval                        loss |  3.50460458\n",
      "Step    220: eval          neg_log_perplexity | -3.50460458\n",
      "Step    220: eval           sequence_accuracy |  0.00000000\n",
      "Step    220: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    220: Finished evaluation\n",
      "\n",
      "Step    230: Ran 10 train steps in 35.87 secs\n",
      "Step    230: Evaluation\n",
      "Step    230: train                   accuracy |  0.16797219\n",
      "Step    230: train                       loss |  3.47752953\n",
      "Step    230: train         neg_log_perplexity | -3.47752953\n",
      "Step    230: train          sequence_accuracy |  0.00000000\n",
      "Step    230: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    230: eval                    accuracy |  0.16840158\n",
      "Step    230: eval                        loss |  3.47556973\n",
      "Step    230: eval          neg_log_perplexity | -3.47556973\n",
      "Step    230: eval           sequence_accuracy |  0.00000000\n",
      "Step    230: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    230: Finished evaluation\n",
      "\n",
      "Step    240: Ran 10 train steps in 35.91 secs\n",
      "Step    240: Evaluation\n",
      "Step    240: train                   accuracy |  0.17628264\n",
      "Step    240: train                       loss |  3.44382095\n",
      "Step    240: train         neg_log_perplexity | -3.44382095\n",
      "Step    240: train          sequence_accuracy |  0.00000000\n",
      "Step    240: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    240: eval                    accuracy |  0.17707378\n",
      "Step    240: eval                        loss |  3.44022584\n",
      "Step    240: eval          neg_log_perplexity | -3.44022584\n",
      "Step    240: eval           sequence_accuracy |  0.00000000\n",
      "Step    240: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    240: Finished evaluation\n",
      "\n",
      "Step    250: Ran 10 train steps in 35.88 secs\n",
      "Step    250: Evaluation\n",
      "Step    250: train                   accuracy |  0.18017170\n",
      "Step    250: train                       loss |  3.41593337\n",
      "Step    250: train         neg_log_perplexity | -3.41593337\n",
      "Step    250: train          sequence_accuracy |  0.00000000\n",
      "Step    250: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    250: eval                    accuracy |  0.18091248\n",
      "Step    250: eval                        loss |  3.41332102\n",
      "Step    250: eval          neg_log_perplexity | -3.41332102\n",
      "Step    250: eval           sequence_accuracy |  0.00000000\n",
      "Step    250: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    250: Finished evaluation\n",
      "\n",
      "Step    260: Ran 10 train steps in 35.86 secs\n",
      "Step    260: Evaluation\n",
      "Step    260: train                   accuracy |  0.18435344\n",
      "Step    260: train                       loss |  3.39523959\n",
      "Step    260: train         neg_log_perplexity | -3.39523959\n",
      "Step    260: train          sequence_accuracy |  0.00000000\n",
      "Step    260: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    260: eval                    accuracy |  0.18420599\n",
      "Step    260: eval                        loss |  3.39600134\n",
      "Step    260: eval          neg_log_perplexity | -3.39600134\n",
      "Step    260: eval           sequence_accuracy |  0.00000000\n",
      "Step    260: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    260: Finished evaluation\n",
      "\n",
      "Step    270: Ran 10 train steps in 35.91 secs\n",
      "Step    270: Evaluation\n",
      "Step    270: train                   accuracy |  0.18911004\n",
      "Step    270: train                       loss |  3.36970568\n",
      "Step    270: train         neg_log_perplexity | -3.36970568\n",
      "Step    270: train          sequence_accuracy |  0.00000000\n",
      "Step    270: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    270: eval                    accuracy |  0.18933046\n",
      "Step    270: eval                        loss |  3.36851001\n",
      "Step    270: eval          neg_log_perplexity | -3.36851001\n",
      "Step    270: eval           sequence_accuracy |  0.00000000\n",
      "Step    270: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    270: Finished evaluation\n",
      "\n",
      "Step    280: Ran 10 train steps in 35.93 secs\n",
      "Step    280: Evaluation\n",
      "Step    280: train                   accuracy |  0.19490740\n",
      "Step    280: train                       loss |  3.33269310\n",
      "Step    280: train         neg_log_perplexity | -3.33269310\n",
      "Step    280: train          sequence_accuracy |  0.00000000\n",
      "Step    280: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    280: eval                    accuracy |  0.19544870\n",
      "Step    280: eval                        loss |  3.33076358\n",
      "Step    280: eval          neg_log_perplexity | -3.33076358\n",
      "Step    280: eval           sequence_accuracy |  0.00000000\n",
      "Step    280: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    280: Finished evaluation\n",
      "\n",
      "Step    290: Ran 10 train steps in 35.94 secs\n",
      "Step    290: Evaluation\n",
      "Step    290: train                   accuracy |  0.19921440\n",
      "Step    290: train                       loss |  3.31151915\n",
      "Step    290: train         neg_log_perplexity | -3.31151915\n",
      "Step    290: train          sequence_accuracy |  0.00000000\n",
      "Step    290: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    290: eval                    accuracy |  0.19932170\n",
      "Step    290: eval                        loss |  3.31043816\n",
      "Step    290: eval          neg_log_perplexity | -3.31043816\n",
      "Step    290: eval           sequence_accuracy |  0.00000000\n",
      "Step    290: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    290: Finished evaluation\n",
      "\n",
      "Step    300: Ran 10 train steps in 35.89 secs\n",
      "Step    300: Evaluation\n",
      "Step    300: train                   accuracy |  0.20518085\n",
      "Step    300: train                       loss |  3.27687955\n",
      "Step    300: train         neg_log_perplexity | -3.27687955\n",
      "Step    300: train          sequence_accuracy |  0.00000000\n",
      "Step    300: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    300: eval                    accuracy |  0.20399268\n",
      "Step    300: eval                        loss |  3.28198123\n",
      "Step    300: eval          neg_log_perplexity | -3.28198123\n",
      "Step    300: eval           sequence_accuracy |  0.00000000\n",
      "Step    300: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    300: Finished evaluation\n",
      "\n",
      "Step    310: Ran 10 train steps in 35.94 secs\n",
      "Step    310: Evaluation\n",
      "Step    310: train                   accuracy |  0.21036319\n",
      "Step    310: train                       loss |  3.24427748\n",
      "Step    310: train         neg_log_perplexity | -3.24427748\n",
      "Step    310: train          sequence_accuracy |  0.00000000\n",
      "Step    310: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    310: eval                    accuracy |  0.21127476\n",
      "Step    310: eval                        loss |  3.23993349\n",
      "Step    310: eval          neg_log_perplexity | -3.23993349\n",
      "Step    310: eval           sequence_accuracy |  0.00000000\n",
      "Step    310: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    310: Finished evaluation\n",
      "\n",
      "Step    320: Ran 10 train steps in 35.96 secs\n",
      "Step    320: Evaluation\n",
      "Step    320: train                   accuracy |  0.21609971\n",
      "Step    320: train                       loss |  3.21452260\n",
      "Step    320: train         neg_log_perplexity | -3.21452260\n",
      "Step    320: train          sequence_accuracy |  0.00000000\n",
      "Step    320: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    320: eval                    accuracy |  0.21669795\n",
      "Step    320: eval                        loss |  3.21138501\n",
      "Step    320: eval          neg_log_perplexity | -3.21138501\n",
      "Step    320: eval           sequence_accuracy |  0.00000000\n",
      "Step    320: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    320: Finished evaluation\n",
      "\n",
      "Step    330: Ran 10 train steps in 35.91 secs\n",
      "Step    330: Evaluation\n",
      "Step    330: train                   accuracy |  0.22236782\n",
      "Step    330: train                       loss |  3.17868233\n",
      "Step    330: train         neg_log_perplexity | -3.17868233\n",
      "Step    330: train          sequence_accuracy |  0.00000000\n",
      "Step    330: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    330: eval                    accuracy |  0.22377764\n",
      "Step    330: eval                        loss |  3.17148757\n",
      "Step    330: eval          neg_log_perplexity | -3.17148757\n",
      "Step    330: eval           sequence_accuracy |  0.00000000\n",
      "Step    330: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    330: Finished evaluation\n",
      "\n",
      "Step    340: Ran 10 train steps in 35.99 secs\n",
      "Step    340: Evaluation\n",
      "Step    340: train                   accuracy |  0.23070478\n",
      "Step    340: train                       loss |  3.13256717\n",
      "Step    340: train         neg_log_perplexity | -3.13256717\n",
      "Step    340: train          sequence_accuracy |  0.00000000\n",
      "Step    340: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    340: eval                    accuracy |  0.22960711\n",
      "Step    340: eval                        loss |  3.13870192\n",
      "Step    340: eval          neg_log_perplexity | -3.13870192\n",
      "Step    340: eval           sequence_accuracy |  0.00000000\n",
      "Step    340: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    340: Finished evaluation\n",
      "\n",
      "Step    350: Ran 10 train steps in 35.98 secs\n",
      "Step    350: Evaluation\n",
      "Step    350: train                   accuracy |  0.23677145\n",
      "Step    350: train                       loss |  3.10378337\n",
      "Step    350: train         neg_log_perplexity | -3.10378337\n",
      "Step    350: train          sequence_accuracy |  0.00000000\n",
      "Step    350: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    350: eval                    accuracy |  0.23833717\n",
      "Step    350: eval                        loss |  3.09448862\n",
      "Step    350: eval          neg_log_perplexity | -3.09448862\n",
      "Step    350: eval           sequence_accuracy |  0.00000000\n",
      "Step    350: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    350: Finished evaluation\n",
      "\n",
      "Step    360: Ran 10 train steps in 36.00 secs\n",
      "Step    360: Evaluation\n",
      "Step    360: train                   accuracy |  0.25070530\n",
      "Step    360: train                       loss |  3.03817987\n",
      "Step    360: train         neg_log_perplexity | -3.03817987\n",
      "Step    360: train          sequence_accuracy |  0.00000000\n",
      "Step    360: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    360: eval                    accuracy |  0.25010511\n",
      "Step    360: eval                        loss |  3.04071903\n",
      "Step    360: eval          neg_log_perplexity | -3.04071903\n",
      "Step    360: eval           sequence_accuracy |  0.00000000\n",
      "Step    360: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    360: Finished evaluation\n",
      "\n",
      "Step    370: Ran 10 train steps in 36.00 secs\n",
      "Step    370: Evaluation\n",
      "Step    370: train                   accuracy |  0.27045715\n",
      "Step    370: train                       loss |  2.94616508\n",
      "Step    370: train         neg_log_perplexity | -2.94616508\n",
      "Step    370: train          sequence_accuracy |  0.00000000\n",
      "Step    370: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    370: eval                    accuracy |  0.27165723\n",
      "Step    370: eval                        loss |  2.94164729\n",
      "Step    370: eval          neg_log_perplexity | -2.94164729\n",
      "Step    370: eval           sequence_accuracy |  0.00000000\n",
      "Step    370: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    370: Finished evaluation\n",
      "\n",
      "Step    380: Ran 10 train steps in 35.99 secs\n",
      "Step    380: Evaluation\n",
      "Step    380: train                   accuracy |  0.29990131\n",
      "Step    380: train                       loss |  2.82829738\n",
      "Step    380: train         neg_log_perplexity | -2.82829738\n",
      "Step    380: train          sequence_accuracy |  0.00000000\n",
      "Step    380: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    380: eval                    accuracy |  0.29907244\n",
      "Step    380: eval                        loss |  2.83281684\n",
      "Step    380: eval          neg_log_perplexity | -2.83281684\n",
      "Step    380: eval           sequence_accuracy |  0.00000000\n",
      "Step    380: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    380: Finished evaluation\n",
      "\n",
      "Step    390: Ran 10 train steps in 36.00 secs\n",
      "Step    390: Evaluation\n",
      "Step    390: train                   accuracy |  0.32804269\n",
      "Step    390: train                       loss |  2.69992661\n",
      "Step    390: train         neg_log_perplexity | -2.69992661\n",
      "Step    390: train          sequence_accuracy |  0.00000000\n",
      "Step    390: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    390: eval                    accuracy |  0.32838470\n",
      "Step    390: eval                        loss |  2.69761896\n",
      "Step    390: eval          neg_log_perplexity | -2.69761896\n",
      "Step    390: eval           sequence_accuracy |  0.00000000\n",
      "Step    390: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    390: Finished evaluation\n",
      "\n",
      "Step    400: Ran 10 train steps in 35.98 secs\n",
      "Step    400: Evaluation\n",
      "Step    400: train                   accuracy |  0.34987044\n",
      "Step    400: train                       loss |  2.59826875\n",
      "Step    400: train         neg_log_perplexity | -2.59826875\n",
      "Step    400: train          sequence_accuracy |  0.00000000\n",
      "Step    400: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    400: eval                    accuracy |  0.34797382\n",
      "Step    400: eval                        loss |  2.60856867\n",
      "Step    400: eval          neg_log_perplexity | -2.60856867\n",
      "Step    400: eval           sequence_accuracy |  0.00000000\n",
      "Step    400: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    400: Finished evaluation\n",
      "\n",
      "Step    410: Ran 10 train steps in 36.01 secs\n",
      "Step    410: Evaluation\n",
      "Step    410: train                   accuracy |  0.36710268\n",
      "Step    410: train                       loss |  2.51607680\n",
      "Step    410: train         neg_log_perplexity | -2.51607680\n",
      "Step    410: train          sequence_accuracy |  0.00000000\n",
      "Step    410: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    410: eval                    accuracy |  0.36785030\n",
      "Step    410: eval                        loss |  2.51273584\n",
      "Step    410: eval          neg_log_perplexity | -2.51273584\n",
      "Step    410: eval           sequence_accuracy |  0.00000000\n",
      "Step    410: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    410: Finished evaluation\n",
      "\n",
      "Step    420: Ran 10 train steps in 36.02 secs\n",
      "Step    420: Evaluation\n",
      "Step    420: train                   accuracy |  0.38284969\n",
      "Step    420: train                       loss |  2.43248701\n",
      "Step    420: train         neg_log_perplexity | -2.43248701\n",
      "Step    420: train          sequence_accuracy |  0.00000000\n",
      "Step    420: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    420: eval                    accuracy |  0.38405955\n",
      "Step    420: eval                        loss |  2.42722297\n",
      "Step    420: eval          neg_log_perplexity | -2.42722297\n",
      "Step    420: eval           sequence_accuracy |  0.00000000\n",
      "Step    420: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    420: Finished evaluation\n",
      "\n",
      "Step    430: Ran 10 train steps in 36.00 secs\n",
      "Step    430: Evaluation\n",
      "Step    430: train                   accuracy |  0.39296305\n",
      "Step    430: train                       loss |  2.38151407\n",
      "Step    430: train         neg_log_perplexity | -2.38151407\n",
      "Step    430: train          sequence_accuracy |  0.00000000\n",
      "Step    430: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    430: eval                    accuracy |  0.39200914\n",
      "Step    430: eval                        loss |  2.38611031\n",
      "Step    430: eval          neg_log_perplexity | -2.38611031\n",
      "Step    430: eval           sequence_accuracy |  0.00000000\n",
      "Step    430: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    430: Finished evaluation\n",
      "\n",
      "Step    440: Ran 10 train steps in 36.08 secs\n",
      "Step    440: Evaluation\n",
      "Step    440: train                   accuracy |  0.40757808\n",
      "Step    440: train                       loss |  2.30677772\n",
      "Step    440: train         neg_log_perplexity | -2.30677772\n",
      "Step    440: train          sequence_accuracy |  0.00000000\n",
      "Step    440: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    440: eval                    accuracy |  0.40809262\n",
      "Step    440: eval                        loss |  2.30505943\n",
      "Step    440: eval          neg_log_perplexity | -2.30505943\n",
      "Step    440: eval           sequence_accuracy |  0.00000000\n",
      "Step    440: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    440: Finished evaluation\n",
      "\n",
      "Step    450: Ran 10 train steps in 36.07 secs\n",
      "Step    450: Evaluation\n",
      "Step    450: train                   accuracy |  0.41764474\n",
      "Step    450: train                       loss |  2.25664473\n",
      "Step    450: train         neg_log_perplexity | -2.25664473\n",
      "Step    450: train          sequence_accuracy |  0.00000000\n",
      "Step    450: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    450: eval                    accuracy |  0.41779193\n",
      "Step    450: eval                        loss |  2.25565338\n",
      "Step    450: eval          neg_log_perplexity | -2.25565338\n",
      "Step    450: eval           sequence_accuracy |  0.00000000\n",
      "Step    450: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    450: Finished evaluation\n",
      "\n",
      "Step    460: Ran 10 train steps in 36.06 secs\n",
      "Step    460: Evaluation\n",
      "Step    460: train                   accuracy |  0.42759588\n",
      "Step    460: train                       loss |  2.20592976\n",
      "Step    460: train         neg_log_perplexity | -2.20592976\n",
      "Step    460: train          sequence_accuracy |  0.00000000\n",
      "Step    460: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    460: eval                    accuracy |  0.42755693\n",
      "Step    460: eval                        loss |  2.20560551\n",
      "Step    460: eval          neg_log_perplexity | -2.20560551\n",
      "Step    460: eval           sequence_accuracy |  0.00000000\n",
      "Step    460: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    460: Finished evaluation\n",
      "\n",
      "Step    470: Ran 10 train steps in 36.61 secs\n",
      "Step    470: Evaluation\n",
      "Step    470: train                   accuracy |  0.43664366\n",
      "Step    470: train                       loss |  2.16188478\n",
      "Step    470: train         neg_log_perplexity | -2.16188478\n",
      "Step    470: train          sequence_accuracy |  0.00000000\n",
      "Step    470: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    470: eval                    accuracy |  0.43731707\n",
      "Step    470: eval                        loss |  2.15834618\n",
      "Step    470: eval          neg_log_perplexity | -2.15834618\n",
      "Step    470: eval           sequence_accuracy |  0.00000000\n",
      "Step    470: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    470: Finished evaluation\n",
      "\n",
      "Step    480: Ran 10 train steps in 35.97 secs\n",
      "Step    480: Evaluation\n",
      "Step    480: train                   accuracy |  0.44642207\n",
      "Step    480: train                       loss |  2.11770296\n",
      "Step    480: train         neg_log_perplexity | -2.11770296\n",
      "Step    480: train          sequence_accuracy |  0.00000000\n",
      "Step    480: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    480: eval                    accuracy |  0.44677722\n",
      "Step    480: eval                        loss |  2.11615348\n",
      "Step    480: eval          neg_log_perplexity | -2.11615348\n",
      "Step    480: eval           sequence_accuracy |  0.00000000\n",
      "Step    480: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    480: Finished evaluation\n",
      "\n",
      "Step    490: Ran 10 train steps in 36.04 secs\n",
      "Step    490: Evaluation\n",
      "Step    490: train                   accuracy |  0.45615032\n",
      "Step    490: train                       loss |  2.06817245\n",
      "Step    490: train         neg_log_perplexity | -2.06817245\n",
      "Step    490: train          sequence_accuracy |  0.00000000\n",
      "Step    490: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    490: eval                    accuracy |  0.45417076\n",
      "Step    490: eval                        loss |  2.07663226\n",
      "Step    490: eval          neg_log_perplexity | -2.07663226\n",
      "Step    490: eval           sequence_accuracy |  0.00000000\n",
      "Step    490: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    490: Finished evaluation\n",
      "\n",
      "Step    500: Ran 10 train steps in 36.04 secs\n",
      "Step    500: Evaluation\n",
      "Step    500: train                   accuracy |  0.46455395\n",
      "Step    500: train                       loss |  2.02813458\n",
      "Step    500: train         neg_log_perplexity | -2.02813458\n",
      "Step    500: train          sequence_accuracy |  0.00000000\n",
      "Step    500: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    500: eval                    accuracy |  0.46431991\n",
      "Step    500: eval                        loss |  2.02890873\n",
      "Step    500: eval          neg_log_perplexity | -2.02890873\n",
      "Step    500: eval           sequence_accuracy |  0.00000000\n",
      "Step    500: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    500: Finished evaluation\n",
      "\n",
      "Step    510: Ran 10 train steps in 36.04 secs\n",
      "Step    510: Evaluation\n",
      "Step    510: train                   accuracy |  0.47449434\n",
      "Step    510: train                       loss |  1.98258090\n",
      "Step    510: train         neg_log_perplexity | -1.98258090\n",
      "Step    510: train          sequence_accuracy |  0.00000000\n",
      "Step    510: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    510: eval                    accuracy |  0.47467047\n",
      "Step    510: eval                        loss |  1.98212230\n",
      "Step    510: eval          neg_log_perplexity | -1.98212230\n",
      "Step    510: eval           sequence_accuracy |  0.00000000\n",
      "Step    510: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    510: Finished evaluation\n",
      "\n",
      "Step    520: Ran 10 train steps in 36.06 secs\n",
      "Step    520: Evaluation\n",
      "Step    520: train                   accuracy |  0.48194185\n",
      "Step    520: train                       loss |  1.94930625\n",
      "Step    520: train         neg_log_perplexity | -1.94930625\n",
      "Step    520: train          sequence_accuracy |  0.00000000\n",
      "Step    520: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    520: eval                    accuracy |  0.48475415\n",
      "Step    520: eval                        loss |  1.93890262\n",
      "Step    520: eval          neg_log_perplexity | -1.93890262\n",
      "Step    520: eval           sequence_accuracy |  0.00000000\n",
      "Step    520: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    520: Finished evaluation\n",
      "\n",
      "Step    530: Ran 10 train steps in 36.06 secs\n",
      "Step    530: Evaluation\n",
      "Step    530: train                   accuracy |  0.48984408\n",
      "Step    530: train                       loss |  1.91204643\n",
      "Step    530: train         neg_log_perplexity | -1.91204643\n",
      "Step    530: train          sequence_accuracy |  0.00000000\n",
      "Step    530: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    530: eval                    accuracy |  0.49224210\n",
      "Step    530: eval                        loss |  1.90163708\n",
      "Step    530: eval          neg_log_perplexity | -1.90163708\n",
      "Step    530: eval           sequence_accuracy |  0.00000000\n",
      "Step    530: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    530: Finished evaluation\n",
      "\n",
      "Step    540: Ran 10 train steps in 36.03 secs\n",
      "Step    540: Evaluation\n",
      "Step    540: train                   accuracy |  0.49659896\n",
      "Step    540: train                       loss |  1.87929368\n",
      "Step    540: train         neg_log_perplexity | -1.87929368\n",
      "Step    540: train          sequence_accuracy |  0.00000000\n",
      "Step    540: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    540: eval                    accuracy |  0.49946409\n",
      "Step    540: eval                        loss |  1.86932325\n",
      "Step    540: eval          neg_log_perplexity | -1.86932325\n",
      "Step    540: eval           sequence_accuracy |  0.00000000\n",
      "Step    540: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    540: Finished evaluation\n",
      "\n",
      "Step    550: Ran 10 train steps in 36.07 secs\n",
      "Step    550: Evaluation\n",
      "Step    550: train                   accuracy |  0.50636226\n",
      "Step    550: train                       loss |  1.83485377\n",
      "Step    550: train         neg_log_perplexity | -1.83485377\n",
      "Step    550: train          sequence_accuracy |  0.00000000\n",
      "Step    550: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    550: eval                    accuracy |  0.50434554\n",
      "Step    550: eval                        loss |  1.84412634\n",
      "Step    550: eval          neg_log_perplexity | -1.84412634\n",
      "Step    550: eval           sequence_accuracy |  0.00000000\n",
      "Step    550: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    550: Finished evaluation\n",
      "\n",
      "Step    560: Ran 10 train steps in 36.06 secs\n",
      "Step    560: Evaluation\n",
      "Step    560: train                   accuracy |  0.51406765\n",
      "Step    560: train                       loss |  1.80493450\n",
      "Step    560: train         neg_log_perplexity | -1.80493450\n",
      "Step    560: train          sequence_accuracy |  0.00000000\n",
      "Step    560: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    560: eval                    accuracy |  0.51431775\n",
      "Step    560: eval                        loss |  1.80395007\n",
      "Step    560: eval          neg_log_perplexity | -1.80395007\n",
      "Step    560: eval           sequence_accuracy |  0.00000000\n",
      "Step    560: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    560: Finished evaluation\n",
      "\n",
      "Step    570: Ran 10 train steps in 36.09 secs\n",
      "Step    570: Evaluation\n",
      "Step    570: train                   accuracy |  0.52220953\n",
      "Step    570: train                       loss |  1.77356899\n",
      "Step    570: train         neg_log_perplexity | -1.77356899\n",
      "Step    570: train          sequence_accuracy |  0.00000000\n",
      "Step    570: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    570: eval                    accuracy |  0.52041245\n",
      "Step    570: eval                        loss |  1.78011560\n",
      "Step    570: eval          neg_log_perplexity | -1.78011560\n",
      "Step    570: eval           sequence_accuracy |  0.00000000\n",
      "Step    570: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    570: Finished evaluation\n",
      "\n",
      "Step    580: Ran 10 train steps in 36.09 secs\n",
      "Step    580: Evaluation\n",
      "Step    580: train                   accuracy |  0.52890265\n",
      "Step    580: train                       loss |  1.74137235\n",
      "Step    580: train         neg_log_perplexity | -1.74137235\n",
      "Step    580: train          sequence_accuracy |  0.00000000\n",
      "Step    580: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    580: eval                    accuracy |  0.52916145\n",
      "Step    580: eval                        loss |  1.74031520\n",
      "Step    580: eval          neg_log_perplexity | -1.74031520\n",
      "Step    580: eval           sequence_accuracy |  0.00000000\n",
      "Step    580: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    580: Finished evaluation\n",
      "\n",
      "Step    590: Ran 10 train steps in 36.06 secs\n",
      "Step    590: Evaluation\n",
      "Step    590: train                   accuracy |  0.53553176\n",
      "Step    590: train                       loss |  1.71244359\n",
      "Step    590: train         neg_log_perplexity | -1.71244359\n",
      "Step    590: train          sequence_accuracy |  0.00000000\n",
      "Step    590: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    590: eval                    accuracy |  0.53437132\n",
      "Step    590: eval                        loss |  1.71717119\n",
      "Step    590: eval          neg_log_perplexity | -1.71717119\n",
      "Step    590: eval           sequence_accuracy |  0.00000000\n",
      "Step    590: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    590: Finished evaluation\n",
      "\n",
      "Step    600: Ran 10 train steps in 36.07 secs\n",
      "Step    600: Evaluation\n",
      "Step    600: train                   accuracy |  0.54124618\n",
      "Step    600: train                       loss |  1.68735898\n",
      "Step    600: train         neg_log_perplexity | -1.68735898\n",
      "Step    600: train          sequence_accuracy |  0.00000000\n",
      "Step    600: train weights_per_batch_per_core |  513812.00000000\n",
      "Step    600: eval                    accuracy |  0.54247957\n",
      "Step    600: eval                        loss |  1.68330669\n",
      "Step    600: eval          neg_log_perplexity | -1.68330669\n",
      "Step    600: eval           sequence_accuracy |  0.00000000\n",
      "Step    600: eval  weights_per_batch_per_core |  513812.00000000\n",
      "Step    600: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "# Train for 600 steps total\n",
    "# The first ~20 steps are slow to run, but after that it reaches steady-state\n",
    "# speed. This will take at least 30 minutes to run to completion, but can safely\n",
    "# be interrupted by selecting \"Runtime > Interrupt Execution\" from the menu.\n",
    "# The language model won't be exceptionally good when trained for just a few\n",
    "# steps and with minimal regularization. However, we can still sample from it to\n",
    "# see what it learns.\n",
    "trainer.train_epoch(n_steps=9, n_eval_steps=1)\n",
    "for _ in range(59):\n",
    "  trainer.train_epoch(n_steps=10, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY3hpgnI5Rgn"
   },
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffeLSbJk35pv",
    "outputId": "6a149fbd-8b55-4e30-d81b-6bb62ae6f71e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we report in the Reformer paper, increasing the number of hashing rounds\n",
    "# helps with quality. We can even increase the number of hashing rounds at\n",
    "# evaluation time only.\n",
    "\n",
    "gin.parse_config(\"\"\"LSHSelfAttention.n_hashes = 4\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "-BwIjdl6_2tX",
    "outputId": "10330139-817c-40f0-93be-79354ba13f1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'CHAPTER I ORts beon, _wo, its not go! seemed stepped short instant. Nastasya, but he did not know it was away. He was a garleep, and he had a flat closed by the hands, and he had not evening, and he had'"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained Reformer in 'predict' mode\n",
    "model = trax.models.ReformerLM(mode='predict')\n",
    "model.init_from_file(os.path.join(output_dir,'model.pkl.gz'),\n",
    "                     weights_only=True)\n",
    "\n",
    "# Sample from ReformerLM\n",
    "output_token_ids = trax.supervised.decoding.autoregressive_sample(\n",
    "    model, temperature=0.0)\n",
    "\n",
    "# Decode token IDs\n",
    "# Reformer outputed a batch with one item, we access it using [0]\n",
    "# tolist() converts from int64 to int, the type SentencePiece expects\n",
    "TOKENIZER.DecodeIds(output_token_ids[0].tolist()) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "udDs_biH0n5U"
   ],
   "name": "Copy of Reformer: Text Generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
